## Abschlusspräsentation Group 6

### Motivation
- Durchbruch bei mathematischen Beweisen mit LLMs und Lean4
- großer Trainingsdatensatz für Python
- Lean4 ist für formelle Beweise geeignet
- beides Open-Source -> gut in der Wissenschaft nutzbar

### Recherche State of the Art
- LLMs erreichen gute Leistung auf Datensätzen mit mathematischen Fragen in verschiedenen mathematischen Disziplinen
- "intuitiv" arbeiten LLMs nur "generativ", sie generieren nur Token nacheinander, und sind keiner Logik befähigt
	- sie werden als "Mittelsmann" genutzt, um logische Aufgaben zu lösen 
	- Beispiel:
		- Python oder Lean4 Code generieren zum Lösen eines Problems
- general-purpose LLMs (allgemeine Fragen) vs. special-purpose LLMs (Coding-Models)

### Hypothese
- Ein general-purpose LLM sollte leichte mathematische Fragen direkt und logisch schlüssig beantworten können und auch Code für leichte Aufgaben generieren können. Es scheitert aber vermutlich an schwierigen math. Problemen, die es weder direkt noch mittels Coding vernünftig lösen kann.
- Ein special purpose LLM scheitert vermutlich eher daran, mathematische Fragen direkt und logisch schlüssig zu beantworten; aber es könnte zumindest Code zum Lösen schwerer Aufgaben generieren.

### Implementierung

Die Implementierung braucht 3 Zutaten:
- einen Datensatz mathematischer Probleme, möglichst mit Ground Truth
- LLM zugreifbar über eine API, damit die Inferenz serverseitig ausgeführt wird
- Automatisierungs-Code für das Loopen über den Datensatz und ggf. Wiederholungen
- als Rückgaben kommen dann LLM-Antworten
  - direkte Antwort: Lösungsweg, Ergebnis
    - Lösungsweg ist direkt interpretierbar, Ergebnis steht direkt da
  - oder Python-Code als Lösungsweg
    - Python-Code muss erst ausgeführt werden in sicherer Umgebung (Sandbox), dann Ergebnis
  - oder Lean4-Code als Lösungsweg
    - Lean4-Code muss erst ausgeführt werden in sicherer Umgebung (hier Sandbox-Server), dann Ergebnis
- die Ergebnisse müssen dann gegen die Ground Truth geprüft werden
  - braucht eine gewisse Flexibilität beim Bewerten, ob die Ergebnisse gleich sind  
    - Variante 1: Vorüberprüfung mit LLM (skaliert gut, vermutlich aber Fehler)
    - Variante 2: manuelle Nachkontrolle (skaliert jedoch schlecht, gute Qualität erreichbar) 


#### Datensatz mathematischer Probleme
- Math500 (typischer Vertreter aus Trainingssätzen)
- 

#### Große Sprachmodelle

#### LLM-API und LLM
- Blablador-Helmholtz
  - frei für die Wissenschaft
  - GPT OSS 120B
    - Open Weights Model zur freien Verwendung
    - populäres LLM
  - GLM-4.7
    - open source
    - leistungsfähiger Coding Agent
- OpenAI API für GPT 5
  - kommerzielles System, nicht offen 
  - LLM: GPT 5.2 mit verschiedenen Reasoning-Stufen

#### Sandbox für Python / Lean4-Compiler auf lokalem Server
- Python Sandbox for safe code execution
- kimina-lean-server for safe and fast code execution

#### Evaluation
- gedankliche Trennung in Lösungsweg und Ergebnis
- Lösungsweg: wird manuell ausgewertet 
- Ergebnisse: automatische Vorprüfung gegen Ground Truth durch ein Sprachmodell 

### Results
- tbd

### Discussion
- Sprachmodelle sind überraschend performant
- Code generieren ist mit einem zero-shot verfahren nicht vielversprechend
	- vergleich mit paper xy mit hoch entwickelter Pipeline und Feedback
- aber: selbst triviale Sachen scheitern teilweise (Berechenbarkeitsdatensatz), das LLM lernt also nicht von den Grundlagen zu immer komplexeren Fragestellungen

## todos
- Timeout von API -> Retry einbauen
- Präsentation und Stichpunkte
- 100 Fragen überprüfen
- evtl. Zusatzdatensatz mit schweren Fragen
